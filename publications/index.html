<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="h_dfHlSBtZBWZvnonl9V5A4ESCnKdWFfGXRYAwBSzyU"> <meta name="msvalidate.01" content="F6B76A94DB2D1F349240696196EFF2E0"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | BIG Culture </title> <meta name="author" content="BIG Culture"> <meta name="description" content="notable publications by BIG Culture members."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta property="og:site_name" content="BIG Culture"> <meta property="og:type" content="website"> <meta property="og:title" content="BIG Culture | publications"> <meta property="og:url" content="https://big-culture.github.io//publications/"> <meta property="og:description" content="notable publications by BIG Culture members."> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="publications"> <meta name="twitter:description" content="notable publications by BIG Culture members."> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "BIG Culture"
        },
        "url": "https://big-culture.github.io//publications/",
        "@type": "WebSite",
        "description": "notable publications by BIG Culture members.",
        "headline": "publications",
        
        "name": "BIG Culture",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon_transparent.ico?8288ce1f3e6bbe1c6c1392ece11e4d8e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://big-culture.github.io//publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">BIG</span> Culture </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/info/">info </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/reading/">reading </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">search <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">notable publications by BIG Culture members.</p> </header> <article> <div class="publications"> In total, BIG Culture has contributed to 20 papers. <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://www.tandfonline.com/toc/rpdm20/current" rel="external nofollow noopener" target="_blank">IJPADM</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Telematic" class="col-sm-8"> <div class="title">Telematic music transmission, resistance and touch</div> <div class="author"> <a href="https://big-culture.github.io/people/tanaka/">Atau Tanaka</a> </div> <div class="periodical"> <em>International Journal of Performance Arts and Digital Media</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1080/14794713.2024.2329836" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://doi.org/10.1080/14794713.2024.2329836" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This article retraces the history of telematic performance from early videophone experiments by the Electronic Café in the 1980s, through a series of ambitious digital art installations at museums like the Centre Pompidou. Onto this history, I map a series of netmusic projects I initiated in this period, from ISDN performances at the Sonar Festival to my installation Global String at Ars Electronica. This sets the context for collaborative online performances held during the COVID pandemic with artists like Paul Sermon and the Chicks on Speed. I finish by describing the Hybrid Live project connecting Goldsmiths and Iklectik Art Labs in London with Stanford University’s CCRMA and SFJazz in California. I describe the low latency audio transport used, the importance of audiovisual synchronisation and the computer vision abstractions resulting in a London-New York remote dance performance. By situating current work in these histories, and closely examining the qualities of the network necessary for the transmission of a sense of embodied experience – and therefore trust – we understand that network performance occurs in its own space, one distinct from physical co-presence. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Telematic</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tanaka, Atau}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Telematic music transmission, resistance and touch}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Performance Arts and Digital Media}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{0}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{0}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-19}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Routledge}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1080/14794713.2024.2329836}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1080/14794713.2024.2329836}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1080/14794713.2024.2329836}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="http://www.computermusicjournal.org/" rel="external nofollow noopener" target="_blank">CMJ</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Electromyogram" class="col-sm-8"> <div class="title">An End-to-End Musical Instrument System That Translates Electromyogram Biosignals to Synthesized Sound</div> <div class="author"> <a href="https://big-culture.github.io/people/tanaka/">Atau Tanaka</a>, Federico Visi, Balandino Di Donato, Martin Klang, and Michael Zbyszyński </div> <div class="periodical"> <em>Computer Music Journal</em>, Jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>This article presents a custom system combining hardware and software that senses physiological signals of the performer’s body resulting from muscle contraction and translates them to computer-synthesized sound. Our goal was to build upon the history of research in the field to develop a complete, integrated system that could be used by nonspecialist musicians. We describe the Embodied AudioVisual Interaction Electromyogram, an end-to-end system spanning wearable sensing on the musician’s body, custom microcontroller-based biosignal acquisition hardware, machine learning–based gesture-to-sound mapping middleware, and software-based granular synthesis sound output. A novel hardware design digitizes the electromyogram signals from the muscle with minimal analog preprocessing and treats it in an audio signal-processing chain as a class-compliant audio and wireless MIDI interface. The mapping layer implements an interactive machine learning workflow in a reinforcement learning configuration and can map gesture features to auditory metadata in a multidimensional information space. The system adapts existing machine learning and synthesis modules to work with the hardware, resulting in an integrated, end-to-end system. We explore its potential as a digital musical instrument through a series of public presentations and concert performances by a range of musical practitioners.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Electromyogram</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tanaka, Atau and Visi, Federico and Donato, Balandino Di and Klang, Martin and Zbyszyński, Michael}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An End-to-End Musical Instrument System That Translates Electromyogram Biosignals to Synthesized Sound}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Music Journal}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{47}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{64-84}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0148-9267}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1162/comj_a_00672}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1162/comj\_a\_00672}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://direct.mit.edu/comj/article-pdf/47/1/64/2386039/comj\_a\_00672.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#e0a23f"> <a href="https://dl.acm.org/conference/chi" rel="external nofollow noopener" target="_blank">CHI</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="SmartHardHat" class="col-sm-8"> <div class="title">Smart Hard Hat: Exploring Shape Changing Hearing Protection</div> <div class="author"> Kwame Dogbe, Henry Glyde, Tim Nguyen, Themis Papathemistocleous, Katie Marquand, and <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter Bennett</a> </div> <div class="periodical"> <em>In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems</em> , Honolulu, HI, USA, Jun 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1145/3334480.3383063" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3334480.3383063" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In this paper we present the Smart Hard Hat; an interactive hard hat that aims to protect the hearing health of construction workers by utilising shape shifting technology. The device responds to loud noises by automatically closing earmuffs around the wearer’s ears, warning them of the damage that is being caused while taking away the need to consciously protect yourself. Construction workers are particularly vulnerable to noise-induced hearing loss, so this was the target user for this design. Initial testing revealed that the Smart Hard Hat effectively blocks out noise, that there is a possibility to expand the design to new user groups, and that there is potential in using shape-changing technologies to protect personal health.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">SmartHardHat</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dogbe, Kwame and Glyde, Henry and Nguyen, Tim and Papathemistocleous, Themis and Marquand, Katie and Bennett, Peter}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Smart Hard Hat: Exploring Shape Changing Hearing Protection}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450368193}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3334480.3383063}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3334480.3383063}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1–6}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{noise-induced hearing loss, shape-changing technology, smart hard hat}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Honolulu, HI, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI EA '20}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#e0a23f"> <a href="https://dl.acm.org/conference/chi" rel="external nofollow noopener" target="_blank">CHI</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Disruptabottle" class="col-sm-8"> <div class="title">Disruptabottle: Encouraging Hydration with an Overflowing Bottle</div> <div class="author"> Adam Beddoe, Romana Burgess, Lucian Carp, Jessica Foster, Adam Fox, Leechay Moran, <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter Bennett</a> , and Daniel Bennett </div> <div class="periodical"> <em>In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems</em> , Honolulu, HI, USA, Jun 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1145/3334480.3382959" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3334480.3382959" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We present a prototype for a targeted behavioural intervention, Disruptabottle, which explores what happens when a ’nudge’ technology becomes a ’shove’ technology. If you do not drink water at a fast enough rate, the bottle will overflow and spill. This reminds the user that they haven’t drunk enough, aggressively nudging them to drink in order to prevent further spillage. This persuasive technology attempts to motivate conscious decision making by drawing attention to the user’s drinking habits. Furthermore, we evaluated the emotions and opinions of potential users towards Disruptabottle, finding that participants generally received the device positively; with 59% reporting that they would use the device and 92% believing it to be an effective way of encouraging healthy drinking habits.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Disruptabottle</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Beddoe, Adam and Burgess, Romana and Carp, Lucian and Foster, Jessica and Fox, Adam and Moran, Leechay and Bennett, Peter and Bennett, Daniel}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Disruptabottle: Encouraging Hydration with an Overflowing Bottle}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450368193}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3334480.3382959}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3334480.3382959}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1–7}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{aversive technology, behaviour change, hydration habits, nudge theory, shove theory}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Honolulu, HI, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI EA '20}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://www.frontiersin.org/" rel="external nofollow noopener" target="_blank">Frontiers</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="HighTechAndTactile" class="col-sm-8"> <div class="title">High-Tech and Tactile: Cognitive Enrichment for Zoo-Housed Gorillas</div> <div class="author"> Fay E. Clark, Stuart I. Gray, <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter Bennett</a>, Lucy J. Mason, and Katy V. Burgess </div> <div class="periodical"> <em>Frontiers in Psychology</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2019.01574/full" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2019.01574/pdf?isPublishedV2=false" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p></p> <p>The field of environmental enrichment for zoo animals, particularly great apes, has been revived by technological advancements such as touchscreen interfaces and motion sensors. However, direct animal-computer interaction (ACI) is impractical or undesirable for many zoos. We developed a modular cuboid puzzle maze for the troop of six Western lowland gorillas (<italic>Gorilla gorilla gorilla</italic>) at Bristol Zoo Gardens, United Kingdom. The gorillas could use their fingers or tools to interact with interconnected modules and remove food rewards. Twelve modules could be interchanged within the frame to create novel iterations with every trial. We took a screen-free approach to enrichment: substituting ACI for tactile, physically complex device components, in addition to hidden automatic sensors, and cameras to log device use. The current study evaluated the gorillas’ behavioral responses to the device, and evaluated it as a form of “cognitive enrichment.” Five out of six gorillas used the device, during monthly trials of 1 h duration, over a 6 month period. All users were female including two infants, and there were significant individual differences in duration of device use. The successful extraction of food rewards was only performed by the three tool-using gorillas. Device use did not diminish over time, and gorillas took turns to use the device alone or as one mother-infant dyad. Our results suggest that the device was a form of cognitive enrichment for the study troop because it allowed gorillas to solve novel challenges, and device use was not associated with behavioral indicators of stress or frustration. However, device exposure had no significant effects on gorilla activity budgets. The device has the potential to be a sustainable enrichment method in the long-term, tailored to individual gorilla skill levels and motivations. Our study represents a technological advancement for gorilla enrichment, an area which had been particularly overlooked until now. We wholly encourage the continued development of this physical maze system for other great apes under human care, with or without computer logging technology.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">HighTechAndTactile</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Clark, Fay E. and Gray, Stuart I. and Bennett, Peter and Mason, Lucy J. and Burgess, Katy V.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{High-Tech and Tactile: Cognitive Enrichment for Zoo-Housed Gorillas}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Frontiers in Psychology}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2019.01574}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3389/fpsyg.2019.01574}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1664-1078}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#34461d"> <a href="https://tei.acm.org/" rel="external nofollow noopener" target="_blank">TEI</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="NewsThings" class="col-sm-8"> <div class="title">NewsThings: Exploring Interdisciplinary IoT News Media Opportunities via User-Centred Design</div> <div class="author"> John Mills, Mark Lochrie, Tom Metcalfe, and <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter Bennett</a> </div> <div class="periodical"> <em>In Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction</em> , Stockholm, Sweden, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1145/3173225.3173267" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3173225.3173267" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Utilising a multidisciplinary and user-centred product and service design approach, ’NewsThings’ explores the potential for domestic and professional internet of things (IoT) objects to convey journalism, media and information. In placing news audiences and industry at the centre of the prototyping process, the project’s web connected objects explore how user requirements may be best met in a perceived post-digital environment. Following a research-through-design methodology and utilising a range of tools - such as workshops, cultural probes, market research and long-term prototype deployment with public and industry, NewsThings aims to generate design insights and prototypes that could position the news media as active participants in the development of IoT products, processes and interactions. This work-in-progress paper outlines the project’s approach, methods, initial findings - up to and including the pre-deployment phase - and focuses on novel insights around user-engagement with news, and the multidisciplinary team’s responses to them.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NewsThings</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mills, John and Lochrie, Mark and Metcalfe, Tom and Bennett, Peter}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{NewsThings: Exploring Interdisciplinary IoT News Media Opportunities via User-Centred Design}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450355681}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3173225.3173267}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3173225.3173267}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{49–56}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{design, insights, interaction, iot, journalism, media, news, objects, personalization, physical, prototype, rtd}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Stockholm, Sweden}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '18}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#e0a23f"> <a href="https://dl.acm.org/conference/chi" rel="external nofollow noopener" target="_blank">CHI</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="CuffLink" class="col-sm-8"> <div class="title">CuffLink: A Wristband to Grab and Release Data Between Devices</div> <div class="author"> Alex Church, Ethan Kenwrick, Yun Park, Luke Hudlass-Galley, Anmol Krishan Sachdeva, Zhiyu Yang, Jess McIntosh, and <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter Bennett</a> </div> <div class="periodical"> <em>In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems</em> , Montreal QC, Canada, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1145/3170427.3188494" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3170427.3188494" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We present CuffLink, a wristband designed to let users transfer files between devices intuitively using grab and release hand gestures. We propose to use ultrasonic transceivers to enable device selection through pointing and employ force-sensitive resistors (FSRs) to detect simple hand gestures. Our prototype demonstration of CuffLink shows that the system can successfully transfer files between two computers using gestures. Preliminary testing with users shows that 83% claim they would use a fully working device over typical sharing methods such as Dropbox and Google Drive. Apart from file sharing, we intend to make CuffLink a re-programmable wearable in future.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">CuffLink</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Church, Alex and Kenwrick, Ethan and Park, Yun and Hudlass-Galley, Luke and Krishan Sachdeva, Anmol and Yang, Zhiyu and McIntosh, Jess and Bennett, Peter}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CuffLink: A Wristband to Grab and Release Data Between Devices}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450356213}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3170427.3188494}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3170427.3188494}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1–6}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{wearable interface, ultrasonic transceivers, gesture recognition, force-sensitive resistors, cross-device files sharing}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Montreal QC, Canada}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI EA '18}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#e0a23f"> <a href="https://dl.acm.org/conference/chi" rel="external nofollow noopener" target="_blank">CHI</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Choptop" class="col-sm-8"> <div class="title">Choptop: An Interactive Chopping Board</div> <div class="author"> Tuana Celik, Orsolya Lukács-Kisbandi, Simon Partridge, Ross Gardiner, Gavin Parker, and <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter Bennett</a> </div> <div class="periodical"> <em>In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems</em> , Montreal QC, Canada, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1145/3170427.3188486" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3170427.3188486" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Choptop is an interactive chopping board that provides simple, relevant recipe guidance and convenient weighing and timing tools for inexperienced cooks. This assistance is particularly useful to individuals such as students who often have limited time to learn how to cook and are therefore drawn towards overpriced and unhealthy alternatives. Step-by-step instructions appear on Choptop’s display, eliminating the need for easily-damaged recipe books and mobile devices. Users navigate Choptop by pressing the chopping surface, which is instrumented with load sensors. Informal testing shows that Choptop may significantly improve the ease and accuracy of following recipes over traditional methods. Users also reported increased enjoyment while following complex recipes.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Choptop</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Celik, Tuana and Luk\'{a}cs-Kisbandi, Orsolya and Partridge, Simon and Gardiner, Ross and Parker, Gavin and Bennett, Peter}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Choptop: An Interactive Chopping Board}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450356213}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3170427.3188486}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3170427.3188486}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1–6}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{smart kitchen, input devices, food preparation, consumer health}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Montreal QC, Canada}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI EA '18}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#34461d"> <a href="https://tei.acm.org/" rel="external nofollow noopener" target="_blank">TEI</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="EchoSnapAndPlayableAle" class="col-sm-8"> <div class="title">EchoSnap and PlayableAle: Exploring Audible Resonant Interaction</div> <div class="author"> <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter Bennett</a>, Christopher Haworth, Gascia Ouzounian, and James Wheale </div> <div class="periodical"> <em>In Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction</em> , Yokohama, Japan, Jun 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1145/3024969.3025091" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3024969.3025091" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This paper presents two projects that use the audible resonance of hollow objects as the basis of novel tangible interaction. EchoSnap explores how an audio feedback loop created between a mobile device’s microphone and speaker can be used to playfully explore and probe the resonant characteristics of hollow objects, using the resulting sounds to control a mobile device. With PlayableAle, the tone made by blowing across the top of a bottle is used as a method of controlling a game. The work presented in this paper builds upon our previous work exploring the concept of resonant bits, where digital information is given resonant properties that can be explored through physical interaction. In particular, this paper expands upon the concept by looking at resonance in the physical as opposed to digital domain. Using EchoSnap and PlayableAle to illustrate we present a preliminary design space for structuring the continuing development of audible resonant interaction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">EchoSnapAndPlayableAle</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bennett, Peter and Haworth, Christopher and Ouzounian, Gascia and Wheale, James}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{EchoSnap and PlayableAle: Exploring Audible Resonant Interaction}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450346764}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3024969.3025091}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3024969.3025091}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{543–549}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{mobile interaction, resonant bits, tangible user interface}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Yokohama, Japan}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '17}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#e0a23f"> <a href="https://dl.acm.org/conference/chi" rel="external nofollow noopener" target="_blank">CHI</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="TanglibleInterfacesForIEC" class="col-sm-8"> <div class="title">Tangible Interfaces for Interactive Evolutionary Computation</div> <div class="author"> Thomas Mitchell, <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter Bennett</a>, Sebastian Madgwick, Edward Davies, and Philip Tew </div> <div class="periodical"> <em>In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</em> , San Jose, California, USA, Jun 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1145/2851581.2892405" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/2851581.2892405" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Interactive evolutionary computation (IEC) is a powerful human-machine optimisation procedure for evolving solutions to complex design problems. In this paper we introduce the novel concept of Tangible Interactive Evolutionary Computation (TIEC), leveraging the benefits of tangible user interfaces to enhance the IEC process and experience to alleviate user fatigue. An example TIEC system is presented and used to evolve biomorph images, with a recreation of the canonical IEC application: The Blind Watchmaker program. An expanded version of the system is also used to design visual states for an atomic visualisation platform called danceroom Spectroscopy, that allows participants to explore quantum phenomena through movement and dance. Initial findings from an informal observational test are presented along with the results from a pilot study to evaluate the potential for TIEC.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">TanglibleInterfacesForIEC</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mitchell, Thomas and Bennett, Peter and Madgwick, Sebastian and Davies, Edward and Tew, Philip}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tangible Interfaces for Interactive Evolutionary Computation}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450340823}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/2851581.2892405}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/2851581.2892405}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2609–2616}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{aesthetic evolution, interactive evolutionary computation, tangible user interface}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{San Jose, California, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI EA '16}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#e0a23f"> <a href="https://dl.acm.org/conference/chi" rel="external nofollow noopener" target="_blank">CHI</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="TangiblesForHealth" class="col-sm-8"> <div class="title">Tangibles for Health Workshop</div> <div class="author"> Audrey Girouard, David McGookin, <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter Bennett</a>, Orit Shaer, Katie A. Siek, and Marilyn Lennon </div> <div class="periodical"> <em>In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</em> , San Jose, California, USA, Jun 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1145/2851581.2856469" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/2851581.2856469" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>eHealth research employing technology and HCI to support wellbeing, recovery and maintenance of conditions, has seen significant progress in recent years. However, such research has primarily focused on mobile "apps" running on commercial smartphones. We believe that Tangible User Interfaces (TUIs) offer many physical and interaction qualities that would benefit the eHealth community. Yet, there is little research that combines the two. Tangibles for Health will bring together leading researchers in tangible user interaction and health to explore the potential of tangibles as applied to healthcare and wellbeing.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">TangiblesForHealth</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Girouard, Audrey and McGookin, David and Bennett, Peter and Shaer, Orit and Siek, Katie A. and Lennon, Marilyn}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tangibles for Health Workshop}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450340823}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/2851581.2856469}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/2851581.2856469}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3461–3468}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{wellbeing, tangible user interfaces, healthcare, e-health, accessibility, HCI}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{San Jose, California, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI EA '16}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#000000"> <a href="https://www.nime.org/" rel="external nofollow noopener" target="_blank">NIME</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ResonantBits1" class="col-sm-8"> <div class="title">Resonant Bits: Controlling Digital Musical Instruments with Resonance and the Ideomotor Effect</div> <div class="author"> <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter Bennett</a>, Jarrod Knibbe, Florent Berthaut, and Kirsten Cater </div> <div class="periodical"> <em>In Proceedings of the International Conference on New Interfaces for Musical Expression</em> , Baton Rouge, Louisiana, USA, Jun 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.nime.org/proceedings/2015/nime2015_235.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Resonant Bits proposes giving digital information resonant dynamic properties, requiring skill and concerted effort for interaction. This paper applies resonant interaction to musical control, exploring musical instruments that are controlled through both purposeful and subconscious resonance. We detail three exploratory prototypes, the first two illustrating the use of resonant gestures and the third focusing on the detection and use of the ideomotor (subconscious micro-movement) effect.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ResonantBits1</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bennett, Peter and Knibbe, Jarrod and Berthaut, Florent and Cater, Kirsten}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Resonant Bits: Controlling Digital Musical Instruments with Resonance and the Ideomotor Effect}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9780692495476}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{The School of Music and the Center for Computation and Technology (CCT), Louisiana State University}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Baton Rouge, Louisiana, USA}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on New Interfaces for Musical Expression}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{176–177}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Baton Rouge, Louisiana, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{NIME 2015}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#e0a23f"> <a href="https://dl.acm.org/conference/chi" rel="external nofollow noopener" target="_blank">CHI</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="TopoTiles" class="col-sm-8"> <div class="title">TopoTiles: Storytelling in Care Homes with Topographic Tangibles</div> <div class="author"> <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter Bennett</a>, Heidi Hinder, Seana Kozar, Christopher Bowdler, Elaine Massung, Tim Cole, Helen Manchester, and Kirsten Cater </div> <div class="periodical"> <em>In Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems</em> , Seoul, Republic of Korea, Jun 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1145/2702613.2732918" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/2702613.2732918" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In this paper we present our initial ethnographic work from developing TopoTiles, Tangible User Interfaces designed to aid storytelling, reminiscence and community building in care homes. Our fieldwork has raised a number of questions which we discuss in this paper including: How can landscape tangibles be used as proxy objects, standing in for landscape and objects unavailable to the storyteller? How can tangible interfaces be used in an indirect or peripheral manner to aid storytelling? Can miniature landscapes aid recollection and story telling through embodied interaction? Are ambiguous depictions conducive to storytelling? Can topographic tangibles encourage inclusivity in group sharing situations? In this paper we share our initial findings to these questions and show how they will inform further TopoTiles design work.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">TopoTiles</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bennett, Peter and Hinder, Heidi and Kozar, Seana and Bowdler, Christopher and Massung, Elaine and Cole, Tim and Manchester, Helen and Cater, Kirsten}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{TopoTiles: Storytelling in Care Homes with Topographic Tangibles}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450331463}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/2702613.2732918}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/2702613.2732918}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{911–916}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{tangible user interface, storytelling, reminiscence}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Seoul, Republic of Korea}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI EA '15}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#e0a23f"> <a href="https://dl.acm.org/conference/chi" rel="external nofollow noopener" target="_blank">CHI</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="FugaciousFilm" class="col-sm-8"> <div class="title">FugaciousFilm: Exploring Attentive Interaction with Ephemeral Material</div> <div class="author"> Hyosun Kwon, Shashank Jaiswal, Steve Benford, Sue Ann Seah, <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter Bennett</a>, Boriana Koleva, and Holger Schnädelbach </div> <div class="periodical"> <em>In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</em> , Seoul, Republic of Korea, Jun 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1145/2702123.2702206" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/2702123.2702206" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces FugaciousFilm, a soap film based touch display, as a platform for Attentive Interaction that encourages the user to be highly focused throughout the use of the interface. Previous work on ephemeral user interfaces has primarily focused on the development of ambient and peripheral displays. In contrast, FugaciousFilm is an ephemeral display that aims to promote highly attentive interaction. We present the iterative process of developing this interface, spanning technical explorations, prototyping and a user study. We report lessons learnt when designing the interface; ranging from the soap film mixture to the impact of frames and apertures. We then describe developing the touch, push, pull and pop interactions. Our user study shows how FugaciousFilm led to focused and attentive interactions during a tournament of enhanced Tic-Tac-Toe. We then finish by discussing how the principles of vulnerability and delicacy can motivate the design of attentive ephemeral interfaces.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">FugaciousFilm</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kwon, Hyosun and Jaiswal, Shashank and Benford, Steve and Seah, Sue Ann and Bennett, Peter and Koleva, Boriana and Schn\"{a}delbach, Holger}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{FugaciousFilm: Exploring Attentive Interaction with Ephemeral Material}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450331456}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/2702123.2702206}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/2702123.2702206}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1285–1294}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{tangible interaction, soap film, non-ambient interaction, ephemeral user interfaces, attentive interaction}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Seoul, Republic of Korea}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI '15}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#34461d"> <a href="https://tei.acm.org/" rel="external nofollow noopener" target="_blank">TEI</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ResonantBits2" class="col-sm-8"> <div class="title">Resonant Bits: Harmonic Interaction with Virtual Pendulums</div> <div class="author"> <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter Bennett</a>, Stuart Nolan, Ved Uttamchandani, Michael Pages, Kirsten Cater, and Mike Fraser </div> <div class="periodical"> <em>In Proceedings of the Ninth International Conference on Tangible, Embedded, and Embodied Interaction</em> , Stanford, California, USA, Jun 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1145/2677199.2680569" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/2677199.2680569" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This paper presents the concept of Resonant Bits, an interaction technique for encouraging engaging, slow and skilful interaction with tangible, mobile and ubiquitous devices. The technique is based on the resonant excitation of harmonic oscillators and allows the exploration of a number of novel types of tangible interaction including: ideomotor control, where subliminal micro-movements accumulate over time to produce a visible outcome; indirect tangible interaction, where a number of devices can be controlled simultaneously through an intermediary object such as a table; and slow interaction, with meditative and repetitive gestures being used for control. The Resonant Bits concept is tested as an interaction method in a study where participants resonate with virtual pendulums on a mobile device. The Harmonic Tuner, a resonance-based music player, is presented as a simple example of using resonant bits. Overall, our ambition in proposing the Resonant Bits concept is to promote skilful, engaging and ultimately rewarding forms of interaction with tangible devices that takes time and patience to learn and master.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ResonantBits2</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bennett, Peter and Nolan, Stuart and Uttamchandani, Ved and Pages, Michael and Cater, Kirsten and Fraser, Mike}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Resonant Bits: Harmonic Interaction with Virtual Pendulums}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450333054}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/2677199.2680569}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/2677199.2680569}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Ninth International Conference on Tangible, Embedded, and Embodied Interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{49–52}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{tangible user interface, slow technology, resonance, ideomotor control, human-computer interaction}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Stanford, California, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '15}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#e0a23f"> <a href="https://dl.acm.org/conference/chi" rel="external nofollow noopener" target="_blank">CHI</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/sensabubble-480.webp 480w,/assets/img/publications/sensabubble-800.webp 800w,/assets/img/publications/sensabubble-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/sensabubble.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sensabubble.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="SensaBubble" class="col-sm-8"> <div class="title">SensaBubble: a chrono-sensory mid-air display of sight and smell</div> <div class="author"> Sue Ann Seah, Diego Martinez Plasencia, <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter D. Bennett</a>, Abhijit Karnik, Vlad Stefan Otrocol, Jarrod Knibbe, Andy Cockburn, and Sriram Subramanian </div> <div class="periodical"> <em>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em> , Toronto, Ontario, Canada, Jun 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1145/2556288.2557087" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/2556288.2557087" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=JGUmf4HqTxM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>We present SensaBubble, a chrono-sensory mid-air display system that generates scented bubbles to deliver information to the user via a number of sensory modalities. The system reliably produces single bubbles of specific sizes along a directed path. Each bubble produced by SensaBubble is filled with fog containing a scent relevant to the notification. The chrono-sensory aspect of SensaBubble means that information is presented both temporally and multimodally. Temporal information is enabled through two forms of persistence: firstly, a visual display projected onto the bubble which only endures until it bursts; secondly, a scent released upon the bursting of the bubble slowly disperses and leaves a longer-lasting perceptible trace of the event. We report details of SensaBubble’s design and implementation, as well as results of technical and user evaluations. We then discuss and demonstrate how SensaBubble can be adapted for use in a wide range of application contexts – from an ambient peripheral display for persistent alerts, to an engaging display for gaming or education.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">SensaBubble</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Seah, Sue Ann and Martinez Plasencia, Diego and Bennett, Peter D. and Karnik, Abhijit and Otrocol, Vlad Stefan and Knibbe, Jarrod and Cockburn, Andy and Subramanian, Sriram}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SensaBubble: a chrono-sensory mid-air display of sight and smell}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450324731}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/2556288.2557087}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/2556288.2557087}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the SIGCHI Conference on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2863–2872}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{multimodality., interactive displays, ephemeral interfaces, bubbles, ambient displays}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Toronto, Ontario, Canada}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI '14}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#efb321"> <a href="https://cscw.acm.org/" rel="external nofollow noopener" target="_blank">CSCW</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="QuickAndDirty" class="col-sm-8"> <div class="title">Quick and dirty: streamlined 3D scanning in archaeology</div> <div class="author"> Jarrod Knibbe, Kenton P. O’Hara, Angeliki Chrysanthi, Mark T. Marshall, <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter D. Bennett</a>, Graeme Earl, Shahram Izadi, and Mike Fraser </div> <div class="periodical"> <em>In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing</em> , Baltimore, Maryland, USA, Jun 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1145/2531602.2531669" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/2531602.2531669" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Capturing data is a key part of archaeological practice, whether for preserving records or to aid interpretation. But the technologies used are complex and expensive, resulting in time-consuming processes associated with their use. These processes force a separation between ongoing interpretive work and capture. Through two field studies we elicit more detail as to what is important about this interpretive work and what might be gained through a closer integration of capture technology with these practices. Drawing on these insights, we go on to present a novel, portable, wireless 3D modeling system that emphasizes "quick and dirty" capture. We discuss its design rational in relation to our field observations and evaluate this rationale further by giving the system to archaeological experts to explore in a variety of settings. While our device compromises on the resolution of traditional 3D scanners, its support of interpretation through emphasis on real-time capture, review and manipulability suggests it could be a valuable tool for the future of archaeology.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">QuickAndDirty</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Knibbe, Jarrod and O'Hara, Kenton P. and Chrysanthi, Angeliki and Marshall, Mark T. and Bennett, Peter D. and Earl, Graeme and Izadi, Shahram and Fraser, Mike}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Quick and dirty: streamlined 3D scanning in archaeology}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450325400}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/2531602.2531669}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/2531602.2531669}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work \&amp; Social Computing}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1366–1376}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{interpretation, archaeology, 3d scanning}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Baltimore, Maryland, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CSCW '14}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2013</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#e0a23f"> <a href="https://dl.acm.org/conference/chi" rel="external nofollow noopener" target="_blank">CHI</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Hapticcues" class="col-sm-8"> <div class="title">Haptic cues: texture as a guide for non-visual tangible interaction</div> <div class="author"> Katrin Wolf, and <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter D. Bennett</a> </div> <div class="periodical"> <em>In CHI ’13 Extended Abstracts on Human Factors in Computing Systems</em> , Paris, France, Jun 2013 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/pdf/10.1145/2468356.2468642" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Tangible User Interfaces (TUIs) represent digital information via a number of sensory modalities including the haptic, visual and auditory senses. We suggest that interaction with tangible interfaces is commonly governed primarily through visual cues, despite the emphasis on tangible representation. We do not doubt that visual feedback offers rich interaction guidance, but argue that emphasis on haptic and auditory feedback could support or substitute vision in situations of visual distraction or impairment. We have developed a series of simple TUIs that allows for the haptic and auditory exploration of visually hidden textures. Our technique is to transmit the force feedback of the texture to the user via the attraction of a ball bearing to a magnet that the user manipulates. This allows the detail of the texture to be presented to the user while visually presenting an entirely flat surface. The use of both opaque and transparent materials allows for controlling the texture visibility for comparative purposes. The resulting Feelable User Interface (FUI), shown in Fig. 1, allows for the exploration of which textures and structures are useful for haptic guidance. The findings of our haptic exploration shall provide basic understanding about the usage of haptic cues for interacting with tangible objects that are visually hidden or are in the user’s visual periphery.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Hapticcues</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wolf, Katrin and Bennett, Peter D.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Haptic cues: texture as a guide for non-visual tangible interaction}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2013}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450319522}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/2468356.2468642}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/2468356.2468642}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{CHI '13 Extended Abstracts on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1599–1604}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{texture, tangible user interface, haptic, guidance}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Paris, France}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI EA '13}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2012</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#34461d"> <a href="https://tei.acm.org/" rel="external nofollow noopener" target="_blank">TEI</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ChronoTape" class="col-sm-8"> <div class="title">ChronoTape: tangible timelines for family history</div> <div class="author"> <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter Bennett</a>, Mike Fraser, and Madeline Balaam </div> <div class="periodical"> <em>In Proceedings of the Sixth International Conference on Tangible, Embedded and Embodied Interaction</em> , Kingston, Ontario, Canada, Jun 2012 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://doi.org/10.1145/2148131.2148144" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://dl.acm.org/doi/pdf/10.1145/2148131.2148144" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>An explosion in the availability of online records has led to surging interest in genealogy. In this paper we explore the present state of genealogical practice, with a particular focus on how the process of research is recorded and later accessed by other researchers. We then present our response, ChronoTape, a novel tangible interface for supporting family history research. The ChronoTape is an example of a temporal tangible interface, an interface designed to enable the tangible representation and control of time. We use the ChronoTape to interrogate the value relationships between physical and digital materials, personal and professional practices, and the ways that records are produced, maintained and ultimately inherited. In contrast to designs that support existing genealogical practice, ChronoTape captures and embeds traces of the researcher within the document of their own research, in three ways: (i) it ensures physical traces of digital research; (ii) it generates personal material around the use of impersonal genealogical data; (iii) it allows for graceful degradation of both its physical and digital components in order to deliberately accommodate the passage of information into the future.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ChronoTape</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bennett, Peter and Fraser, Mike and Balaam, Madeline}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ChronoTape: tangible timelines for family history}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2012}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450311748}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/2148131.2148144}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/2148131.2148144}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Sixth International Conference on Tangible, Embedded and Embodied Interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{49–56}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{temporal tangible user interfaces, personalisation, inheritance, genealogy}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Kingston, Ontario, Canada}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{TEI '12}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2008</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#59fda9"> <a href="https://nordichi.eu/" rel="external nofollow noopener" target="_blank">NordiCHI</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publications/paper-icon-blank-480.webp 480w,/assets/img/publications/paper-icon-blank-800.webp 800w,/assets/img/publications/paper-icon-blank-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publications/paper-icon-blank.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="paper-icon-blank.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bennett_beatbearing_2008" class="col-sm-8"> <div class="title">The BeatBearing: a Tangible Rhythm Sequencer</div> <div class="author"> <a href="https://www.peteinfo.com/" rel="external nofollow noopener" target="_blank">Peter Bennett</a>, and Sile O’Modhrain </div> <div class="periodical"> <em>In Electronic Proceedings of NordiCHI ’08: 5th Nordic Conference on Computer-Human Interaction</em> , Jun 2008 </div> <div class="periodical"> event-place: Lund, Sweden </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>The BeatBearing is a novel musical instrument that allows users to manipulate rhythmic patterns through the placement of ball bearings on a grid. The BeatBearing has been developed as an explorative design case for investigating how the theory of embodied interaction can inform the design of new digital musical instruments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bennett_beatbearing_2008</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The {BeatBearing}: a {Tangible} {Rhythm} {Sequencer}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Electronic {Proceedings} of {NordiCHI} '08: 5th {Nordic} {Conference} on {Computer}-{Human} {Interaction}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bennett, Peter and O'Modhrain, Sile}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2008}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{event-place: Lund, Sweden}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 BIG Culture. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=document.querySelector(".navbar-collapse");e.classList.contains("show")&&e.classList.remove("show"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-",title:"",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-info",title:"info",description:"",section:"Navigation",handler:()=>{window.location.href="/info/"}},{id:"nav-people",title:"people",description:"staff and students engaged in BIG-Culture",section:"Navigation",handler:()=>{window.location.href="/people/"}},{id:"nav-projects",title:"projects",description:"We&#39;re working a variety of interdisciplinary projects combining technology and the arts.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"publications",description:"notable publications by BIG Culture members.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-reading",title:"reading",description:"suggested reading by BIG Culture members.",section:"Navigation",handler:()=>{window.location.href="/reading/"}},{id:"nav-repositories",title:"repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"post-ron-herrema-talk",title:"Ron Herrema talk",description:"Today we had a talk from Ron Herrema on a phenomenological approach to writing computer code",section:"Posts",handler:()=>{window.location.href="/2024/ron-herrema/"}},{id:"post-intersensory-data-representations-of-a-black-hole",title:"Intersensory data representations of a black hole",description:"Interesting reading about Dr. Kyle Keane&#39;s research",section:"Posts",handler:()=>{window.location.href="/2024/black-hole/"}},{id:"news-big-culture-website-now-live",title:"BIG-Culture website now live",description:"",section:"News"},{id:"news-the-big-non-away-day-is-on-thursday-13th-june-at-13-30-in-mvb-1-11",title:"The BIG Non-Away Day is on Thursday 13th June, at 13:30 in MVB 1.11",description:"",section:"News"},{id:"news-today-ron-herrema-will-be-giving-a-talk-at-14-00-in-mvb-2-19",title:"Today Ron Herrema will be giving a talk at 14:00 in MVB 2.19",description:"",section:"News"},{id:"news-quick-reminder-that-the-big-non-away-day-is-today-at-13-30-in-mvb-1-11",title:"Quick reminder that the BIG Non-Away Day is today, at 13:30 in MVB 1.11",description:"",section:"News"},{id:"news-atau-will-be-giving-a-talk-on-a-late-breaking-paper-in-the-big-meeting-on-thursday",title:"Atau will be giving a talk on a late-breaking paper in the BIG Meeting on Thursday.",description:"",section:"News"},{id:"people-pete-bennett",title:"Pete Bennett",description:"Lecturer, interested in playful interfaces, tangible interaction and generative music.",section:"People",handler:()=>{window.location.href="/people/bennett/"}},{id:"people-jack-burnett",title:"Jack Burnett",description:"Interactive AI CDT Student",section:"People",handler:()=>{window.location.href="/people/burnett/"}},{id:"people-vishal-joshi",title:"Vishal Joshi",description:"Interactive AI CDT Student",section:"People",handler:()=>{window.location.href="/people/joshi/"}},{id:"people-atau-tanaka",title:"Atau Tanaka",description:"embodied musical interaction, human computer interaction and gestural computer music performance.",section:"People",handler:()=>{window.location.href="/people/tanaka/"}},{id:"people-jacob-thomas",title:"Jacob Thomas",description:"PhD student researching immersive arts and technologies in Bristol Interaction Group",section:"People",handler:()=>{window.location.href="/people/thomas/"}},{id:"people-nathaniel-thorne",title:"Nathaniel Thorne",description:"Immersive Arts Student",section:"People",handler:()=>{window.location.href="/people/thorne/"}},{id:"people-mamoru-watanabe",title:"Mamoru Watanabe",description:"PhD student at Bristol Interaction Group",section:"People",handler:()=>{window.location.href="/people/watanabe/"}},{id:"posts-intersensory-data-representations-of-a-black-hole",title:"Intersensory data representations of a black hole",description:"Interesting reading about Dr. Kyle Keane&#39;s research",section:"Posts",handler:()=>{window.location.href="/2024/black-hole/"}},{id:"posts-ron-herrema-talk",title:"Ron Herrema talk",description:"Today we had a talk from Ron Herrema on a phenomenological approach to writing computer code",section:"Posts",handler:()=>{window.location.href="/2024/ron-herrema/"}},{id:"projects-16x16",title:"16x16",description:"An audiovisual sequencer installation",section:"Projects",handler:()=>{window.location.href="/projects/16x16/"}},{id:"projects-aimc-workshop",title:"AIMC workshop",description:"A Synth made of Chicken Nuggets you Play with your Elbows - Exploring AI-supported Musical Instrument Design",section:"Projects",handler:()=>{window.location.href="/projects/AIMC_workshop/"}},{id:"projects-improvcues",title:"improvcues",description:"improvisation with generative oblique strategies",section:"Projects",handler:()=>{window.location.href="/projects/improvCues/"}},{id:"projects-music-hack-club",title:"Music Hack Club",description:"A weekly meeting to make noise.",section:"Projects",handler:()=>{window.location.href="/projects/music_hack_club/"}},{id:"projects-research-on-synaesthesia",title:"Research on Synaesthesia",description:"Research on synaesthesia",section:"Projects",handler:()=>{window.location.href="/projects/synaesthesia_study/"}},{id:"projects-investigating-the-effects-of-language-model-player-assistants-on-the-social-dynamics-of-table-top-role-playing-game-groups",title:"Investigating the effects of language model player assistants on the social dynamics of table-top role-playing game groups",description:"",section:"Projects",handler:()=>{window.location.href="/projects/vishal_summer_project/"}},{id:"projects-website-design",title:"Website Design",description:"help design the BIG Culture Website",section:"Projects",handler:()=>{window.location.href="/projects/website_design/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%70%65%74%65%72.%62%65%6E%6E%65%74%74@%62%72%69%73%74%6F%6C.%61%63.%75%6B","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> </body> </html>